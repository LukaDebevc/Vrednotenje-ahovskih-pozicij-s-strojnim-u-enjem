{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a12d17-6e5b-46f9-b453-6c293a558c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "class ChessEpochDataLoader:\n",
    "    def __init__(self, folder_path, device, batch_size=32):\n",
    "        self.folder_path = folder_path\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_files = self.get_batch_files()\n",
    "\n",
    "    def get_batch_files(self):\n",
    "        batch_files = []\n",
    "        for file in os.listdir(self.folder_path):\n",
    "            if file.startswith(\"encoding_\") and file.endswith(\".npz\"):\n",
    "                batch_num = file.split(\"_\")[1].split(\".\")[0]\n",
    "                batch_files.append(batch_num)\n",
    "        return sorted(batch_files)\n",
    "\n",
    "    def load_batch(self, batch_num):\n",
    "        # print(f\"Loading batch {batch_num}\")\n",
    "        encodings = sparse.load_npz(os.path.join(self.folder_path, f\"encoding_{batch_num}.npz\"))\n",
    "        to_move = np.load(os.path.join(self.folder_path, f\"to_move_{batch_num}.npy\"))\n",
    "        outcomes = np.load(os.path.join(self.folder_path, f\"outcomes_{batch_num}.npy\"))\n",
    "\n",
    "        encodings = torch.from_numpy(encodings.todense()).float().to(self.device)\n",
    "        encodings = encodings.view(-1, 2, 6, 8, 8)\n",
    "        to_move = torch.from_numpy(to_move).float().to(self.device)\n",
    "        outcomes = torch.from_numpy(outcomes).float().to(self.device)\n",
    "\n",
    "        # Flip boards where black is to move\n",
    "        black_to_move = to_move == 0\n",
    "        encodings[black_to_move] = encodings[black_to_move].flip(1)\n",
    "        encodings[black_to_move] = encodings[black_to_move].flip(3)\n",
    "        outcomes[black_to_move] = 1 - outcomes[black_to_move]\n",
    "\n",
    "        return encodings, outcomes\n",
    "\n",
    "    def __iter__(self):\n",
    "        # random.shuffle(self.batch_files)  # Shuffle batch order for each epoch\n",
    "        is_first = 0\n",
    "        while True:\n",
    "            for batch_num in self.batch_files:\n",
    "                encodings, outcomes = self.load_batch(batch_num)\n",
    "                for i in range(0, len(encodings), self.batch_size):\n",
    "                    batch_encodings = encodings[i:i + self.batch_size]\n",
    "                    batch_outcomes = outcomes[i:i + self.batch_size]\n",
    "\n",
    "                    yield batch_encodings, batch_outcomes\n",
    "                # Clear the current batch from GPU memory\n",
    "                del encodings, outcomes\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec6e171-4889-4781-a78e-5903cc37ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleChessNet(nn.Module):\n",
    "    def __init__(self, acc_size):\n",
    "        super(SimpleChessNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, acc_size)\n",
    "        self.fc2 = nn.Linear(acc_size, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_1):\n",
    "        x_1 = x_1.view(-1, 768)\n",
    "        x = self.fc1(x_1).clamp(0, 1)\n",
    "        x = self.fc2(x).clamp(0, 1)\n",
    "        x = self.fc3(x).clamp(0, 1)\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96c5826-702d-4bb2-a5fb-a85a53cd152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs, learning_rate, gamma, device):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        s_loss = 0\n",
    "        for i, (player_view, labels) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(player_view)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            s_loss += loss.item()\n",
    "            if i % 10000 == 0:\n",
    "                if i % 120000 == 0 and i != 0:\n",
    "                    print(f\"  Batch {i}, Loss: {s_loss / 10000:.5f}\")\n",
    "                s_loss = 0\n",
    "                # torch.save(model.state_dict(), f'models/pre_train_{i}.pth')\n",
    "                scheduler.step()\n",
    "            if i == 360000:\n",
    "                break ## zadnih 10000 bacthov (320_000 primerov) je testnih. podatke samo enkrat uporabim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cbbf712-4cf3-48af-968b-7b8a35c6ce00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with input_size=32, learning_rate=0.001, gamma=0.99, starting_time=0.03031444549560547\n",
      "  Batch 120000, Loss: 0.10415\n",
      "  Batch 240000, Loss: 0.10300\n",
      "  Batch 360000, Loss: 0.10135\n",
      "Training model with input_size=32, learning_rate=0.001, gamma=0.95, starting_time=386.5256402492523\n",
      "  Batch 120000, Loss: 0.10289\n",
      "  Batch 240000, Loss: 0.10097\n",
      "  Batch 360000, Loss: 0.09902\n",
      "Training model with input_size=32, learning_rate=0.001, gamma=0.9, starting_time=772.0853114128113\n",
      "  Batch 120000, Loss: 0.10210\n",
      "  Batch 240000, Loss: 0.10044\n",
      "  Batch 360000, Loss: 0.09912\n",
      "Training model with input_size=32, learning_rate=0.0005, gamma=0.99, starting_time=1157.6811785697937\n",
      "  Batch 120000, Loss: 0.10389\n",
      "  Batch 240000, Loss: 0.10219\n",
      "  Batch 360000, Loss: 0.10047\n",
      "Training model with input_size=32, learning_rate=0.0005, gamma=0.95, starting_time=1543.3803930282593\n",
      "  Batch 120000, Loss: 0.10294\n",
      "  Batch 240000, Loss: 0.10107\n",
      "  Batch 360000, Loss: 0.09914\n",
      "Training model with input_size=32, learning_rate=0.0005, gamma=0.9, starting_time=1928.3799510002136\n",
      "  Batch 120000, Loss: 0.10208\n",
      "  Batch 240000, Loss: 0.10052\n",
      "  Batch 360000, Loss: 0.09930\n",
      "Training model with input_size=32, learning_rate=0.00025, gamma=0.99, starting_time=2313.21497797966\n",
      "  Batch 120000, Loss: 0.10389\n",
      "  Batch 240000, Loss: 0.10224\n",
      "  Batch 360000, Loss: 0.10048\n",
      "Training model with input_size=32, learning_rate=0.00025, gamma=0.95, starting_time=2699.7053451538086\n",
      "  Batch 120000, Loss: 0.10337\n",
      "  Batch 240000, Loss: 0.10172\n",
      "  Batch 360000, Loss: 0.09998\n",
      "Training model with input_size=32, learning_rate=0.00025, gamma=0.9, starting_time=3083.4879422187805\n",
      "  Batch 120000, Loss: 0.10269\n",
      "  Batch 240000, Loss: 0.10154\n",
      "  Batch 360000, Loss: 0.10041\n",
      "Training model with input_size=32, learning_rate=0.000125, gamma=0.99, starting_time=3470.3952498435974\n",
      "  Batch 120000, Loss: 0.10470\n",
      "  Batch 240000, Loss: 0.10351\n",
      "  Batch 360000, Loss: 0.10167\n",
      "Training model with input_size=32, learning_rate=0.000125, gamma=0.95, starting_time=3856.323358774185\n",
      "  Batch 120000, Loss: 0.10409\n",
      "  Batch 240000, Loss: 0.10322\n",
      "  Batch 360000, Loss: 0.10178\n",
      "Training model with input_size=32, learning_rate=0.000125, gamma=0.9, starting_time=4242.0765290260315\n",
      "  Batch 120000, Loss: 0.10330\n",
      "  Batch 240000, Loss: 0.10284\n",
      "  Batch 360000, Loss: 0.10181\n",
      "Training model with input_size=32, learning_rate=6.25e-05, gamma=0.99, starting_time=4627.831174612045\n",
      "  Batch 120000, Loss: 0.10449\n",
      "  Batch 240000, Loss: 0.10421\n",
      "  Batch 360000, Loss: 0.10275\n",
      "Training model with input_size=32, learning_rate=6.25e-05, gamma=0.95, starting_time=5014.481522321701\n",
      "  Batch 120000, Loss: 0.10398\n",
      "  Batch 240000, Loss: 0.10362\n",
      "  Batch 360000, Loss: 0.10240\n",
      "Training model with input_size=32, learning_rate=6.25e-05, gamma=0.9, starting_time=5407.504853725433\n",
      "  Batch 120000, Loss: 0.10345\n",
      "  Batch 240000, Loss: 0.10328\n",
      "  Batch 360000, Loss: 0.10226\n",
      "Training model with input_size=512, learning_rate=0.001, gamma=0.99, starting_time=5791.531660079956\n",
      "  Batch 120000, Loss: 0.10529\n",
      "  Batch 240000, Loss: 0.10407\n",
      "  Batch 360000, Loss: 0.10228\n",
      "Training model with input_size=512, learning_rate=0.001, gamma=0.95, starting_time=6186.084256649017\n",
      "  Batch 120000, Loss: 0.10314\n",
      "  Batch 240000, Loss: 0.10054\n",
      "  Batch 360000, Loss: 0.09802\n",
      "Training model with input_size=512, learning_rate=0.001, gamma=0.9, starting_time=6579.215052843094\n",
      "  Batch 120000, Loss: 0.10109\n",
      "  Batch 240000, Loss: 0.09868\n",
      "  Batch 360000, Loss: 0.09712\n",
      "Training model with input_size=512, learning_rate=0.0005, gamma=0.99, starting_time=6973.531269311905\n",
      "  Batch 120000, Loss: 0.10296\n",
      "  Batch 240000, Loss: 0.10125\n",
      "  Batch 360000, Loss: 0.09935\n",
      "Training model with input_size=512, learning_rate=0.0005, gamma=0.95, starting_time=7366.348478794098\n",
      "  Batch 120000, Loss: 0.10141\n",
      "  Batch 240000, Loss: 0.09888\n",
      "  Batch 360000, Loss: 0.09661\n",
      "Training model with input_size=512, learning_rate=0.0005, gamma=0.9, starting_time=7754.857791662216\n",
      "  Batch 120000, Loss: 0.10009\n",
      "  Batch 240000, Loss: 0.09784\n",
      "  Batch 360000, Loss: 0.09639\n",
      "Training model with input_size=512, learning_rate=0.00025, gamma=0.99, starting_time=8144.027634143829\n",
      "  Batch 120000, Loss: 0.10235\n",
      "  Batch 240000, Loss: 0.10045\n",
      "  Batch 360000, Loss: 0.09840\n",
      "Training model with input_size=512, learning_rate=0.00025, gamma=0.95, starting_time=8532.629513502121\n",
      "  Batch 120000, Loss: 0.10123\n",
      "  Batch 240000, Loss: 0.09878\n",
      "  Batch 360000, Loss: 0.09658\n",
      "Training model with input_size=512, learning_rate=0.00025, gamma=0.9, starting_time=8921.458112955093\n",
      "  Batch 120000, Loss: 0.10022\n",
      "  Batch 240000, Loss: 0.09814\n",
      "  Batch 360000, Loss: 0.09671\n",
      "Training model with input_size=512, learning_rate=0.000125, gamma=0.99, starting_time=9309.498345375061\n",
      "  Batch 120000, Loss: 0.10313\n",
      "  Batch 240000, Loss: 0.10100\n",
      "  Batch 360000, Loss: 0.09878\n",
      "Training model with input_size=512, learning_rate=0.000125, gamma=0.95, starting_time=9698.85512971878\n",
      "  Batch 120000, Loss: 0.10190\n",
      "  Batch 240000, Loss: 0.09944\n",
      "  Batch 360000, Loss: 0.09742\n",
      "Training model with input_size=512, learning_rate=0.000125, gamma=0.9, starting_time=10087.194787740707\n",
      "  Batch 120000, Loss: 0.10163\n",
      "  Batch 240000, Loss: 0.09983\n",
      "  Batch 360000, Loss: 0.09867\n",
      "Training model with input_size=512, learning_rate=6.25e-05, gamma=0.99, starting_time=10474.029029130936\n",
      "  Batch 120000, Loss: 0.10361\n",
      "  Batch 240000, Loss: 0.10169\n",
      "  Batch 360000, Loss: 0.09961\n",
      "Training model with input_size=512, learning_rate=6.25e-05, gamma=0.95, starting_time=10860.524701356888\n",
      "  Batch 120000, Loss: 0.10292\n",
      "  Batch 240000, Loss: 0.10118\n",
      "  Batch 360000, Loss: 0.09952\n",
      "Training model with input_size=512, learning_rate=6.25e-05, gamma=0.9, starting_time=11249.571570634842\n",
      "  Batch 120000, Loss: 0.10245\n",
      "  Batch 240000, Loss: 0.10144\n",
      "  Batch 360000, Loss: 0.10044\n",
      "Training model with input_size=8192, learning_rate=0.001, gamma=0.99, starting_time=11638.364385843277\n",
      "  Batch 120000, Loss: 0.11862\n",
      "  Batch 240000, Loss: 0.11714\n",
      "  Batch 360000, Loss: 0.11446\n",
      "Training model with input_size=8192, learning_rate=0.001, gamma=0.95, starting_time=12606.687049627304\n",
      "  Batch 120000, Loss: 0.11320\n",
      "  Batch 240000, Loss: 0.10929\n",
      "  Batch 360000, Loss: 0.10566\n",
      "Training model with input_size=8192, learning_rate=0.001, gamma=0.9, starting_time=13573.138891220093\n",
      "  Batch 120000, Loss: 0.11304\n",
      "  Batch 240000, Loss: 0.10879\n",
      "  Batch 360000, Loss: 0.10579\n",
      "Training model with input_size=8192, learning_rate=0.0005, gamma=0.99, starting_time=14534.759602069855\n",
      "  Batch 120000, Loss: 0.11276\n",
      "  Batch 240000, Loss: 0.11149\n",
      "  Batch 360000, Loss: 0.10918\n",
      "Training model with input_size=8192, learning_rate=0.0005, gamma=0.95, starting_time=15508.700754642487\n",
      "  Batch 120000, Loss: 0.10823\n",
      "  Batch 240000, Loss: 0.10446\n",
      "  Batch 360000, Loss: 0.10122\n",
      "Training model with input_size=8192, learning_rate=0.0005, gamma=0.9, starting_time=16499.167484521866\n",
      "  Batch 120000, Loss: 0.10618\n",
      "  Batch 240000, Loss: 0.10171\n",
      "  Batch 360000, Loss: 0.09964\n",
      "Training model with input_size=8192, learning_rate=0.00025, gamma=0.99, starting_time=17481.4172642231\n",
      "  Batch 120000, Loss: 0.10647\n",
      "  Batch 240000, Loss: 0.10481\n",
      "  Batch 360000, Loss: 0.10288\n",
      "Training model with input_size=8192, learning_rate=0.00025, gamma=0.95, starting_time=18479.660713672638\n",
      "  Batch 120000, Loss: 0.10248\n",
      "  Batch 240000, Loss: 0.09950\n",
      "  Batch 360000, Loss: 0.09676\n",
      "Training model with input_size=8192, learning_rate=0.00025, gamma=0.9, starting_time=19471.797504901886\n",
      "  Batch 120000, Loss: 0.10030\n",
      "  Batch 240000, Loss: 0.09753\n",
      "  Batch 360000, Loss: 0.09599\n",
      "Training model with input_size=8192, learning_rate=0.000125, gamma=0.99, starting_time=20446.09720468521\n",
      "  Batch 120000, Loss: 0.10255\n",
      "  Batch 240000, Loss: 0.10061\n",
      "  Batch 360000, Loss: 0.09860\n",
      "Training model with input_size=8192, learning_rate=0.000125, gamma=0.95, starting_time=21454.04437327385\n",
      "  Batch 120000, Loss: 0.10107\n",
      "  Batch 240000, Loss: 0.09843\n",
      "  Batch 360000, Loss: 0.09609\n",
      "Training model with input_size=8192, learning_rate=0.000125, gamma=0.9, starting_time=22416.748893499374\n",
      "  Batch 120000, Loss: 0.09980\n",
      "  Batch 240000, Loss: 0.09754\n",
      "  Batch 360000, Loss: 0.09607\n",
      "Training model with input_size=8192, learning_rate=6.25e-05, gamma=0.99, starting_time=23379.908126831055\n",
      "  Batch 120000, Loss: 0.10236\n",
      "  Batch 240000, Loss: 0.10010\n",
      "  Batch 360000, Loss: 0.09791\n",
      "Training model with input_size=8192, learning_rate=6.25e-05, gamma=0.95, starting_time=24342.65856409073\n",
      "  Batch 120000, Loss: 0.10223\n",
      "  Batch 240000, Loss: 0.09964\n",
      "  Batch 360000, Loss: 0.09745\n",
      "Training model with input_size=8192, learning_rate=6.25e-05, gamma=0.9, starting_time=25308.714351177216\n",
      "  Batch 120000, Loss: 0.10095\n",
      "  Batch 240000, Loss: 0.09889\n",
      "  Batch 360000, Loss: 0.09759\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    folder_path = \"/home/luka/PycharmProjects/pythonProject/diplomska/nets\"  # Replace with your data folder path\n",
    "    data_loader = ChessEpochDataLoader(folder_path, device, batch_size=32)\n",
    "\n",
    "    num_epochs = 1\n",
    "    learning_rates = [0.001 * 0.5 ** i for i in range(5)]\n",
    "    gammas = [0.99, 0.95, 0.9]\n",
    "    input_sizes = [32, 32 * 16, 32 * 16 * 16]  # [32, 512, 4096]\n",
    "    for input_size in input_sizes:\n",
    "        for lr in learning_rates:\n",
    "            for gamma in gammas:\n",
    "                print(f\"Training model with input_size={input_size}, learning_rate={lr}, gamma={gamma}, starting_time={time.time()-t0}\")\n",
    "                model = SimpleChessNet(input_size).to(device)\n",
    "                # checkpoint_path = r\"/home/luka/PycharmProjects/pythonProject/diplomska/nets/models/pre_train_360000.pth\"\n",
    "                # checkpoint = torch.load(checkpoint_path)\n",
    "                model.train()\n",
    "                train(model, data_loader, num_epochs, lr, gamma, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
